{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Sh1tCsCI-kId"],"toc_visible":true,"authorship_tag":"ABX9TyM+aD1wHUIiH9NQqy2TuaFc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# セットアップ"],"metadata":{"id":"Zcb6VGGmvXJy"}},{"cell_type":"code","source":["!sudo apt-get update\n","!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n","!pip install 'imageio==2.4.0'\n","!pip install pyvirtualdisplay\n","!pip install tf-agents[reverb]\n","!pip install pyglet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"khVTamwZw5NZ","executionInfo":{"status":"ok","timestamp":1724302650087,"user_tz":-540,"elapsed":162260,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}},"outputId":"946e71f0-6761-4ef8-d1b3-0c254350ac56"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Ign:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n","Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n","Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,944 kB]\n","Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,223 kB]\n","Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,135 kB]\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,221 kB]\n","Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,555 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,041 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,497 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,424 kB]\n","Fetched 24.4 MB in 11s (2,196 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","The following additional packages will be installed:\n","  freeglut3 libegl-dev libfontenc1 libgl-dev libgl1-mesa-dev libgles-dev\n","  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n","  libglx-dev libice-dev libopengl-dev libsm-dev libxfont2 libxkbfile1\n","  libxt-dev x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","Suggested packages:\n","  libice-doc libsm-doc libxt-doc\n","The following NEW packages will be installed:\n","  freeglut3 freeglut3-dev libegl-dev libfontenc1 libgl-dev libgl1-mesa-dev\n","  libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\n","  libglvnd-dev libglx-dev libice-dev libopengl-dev libsm-dev libxfont2\n","  libxkbfile1 libxt-dev x11-xkb-utils xfonts-base xfonts-encodings\n","  xfonts-utils xserver-common xvfb\n","0 upgraded, 25 newly installed, 0 to remove and 47 not upgraded.\n","Need to get 9,075 kB of archives.\n","After this operation, 18.7 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.2 [6,842 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.11 [28.6 kB]\n","Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.11 [863 kB]\n","Fetched 9,075 kB in 3s (3,440 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 25.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 123595 files and directories currently installed.)\n","Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libglx-dev:amd64.\n","Preparing to unpack .../01-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../02-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../04-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../05-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../06-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libice-dev:amd64.\n","Preparing to unpack .../12-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n","Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n","Selecting previously unselected package libsm-dev:amd64.\n","Preparing to unpack .../13-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n","Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Selecting previously unselected package libxt-dev:amd64.\n","Preparing to unpack .../14-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n","Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n","Selecting previously unselected package freeglut3-dev:amd64.\n","Preparing to unpack .../15-freeglut3-dev_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libfontenc1:amd64.\n","Preparing to unpack .../16-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../17-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../18-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../19-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../20-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../21-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../22-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../23-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.11_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../24-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.11_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n","Setting up freeglut3:amd64 (2.8.1-6) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n","Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n","Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Collecting imageio==2.4.0\n","  Downloading imageio-2.4.0.tar.gz (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.0) (1.26.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.0) (9.4.0)\n","Building wheels for collected packages: imageio\n","  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imageio: filename=imageio-2.4.0-py3-none-any.whl size=3303894 sha256=ccf96607ee117ac4253cfd6d8a7c3da1b7ab4185d534dab7a8ed37755a1b1a9e\n","  Stored in directory: /root/.cache/pip/wheels/8a/d8/1c/eb9350a4bd3ae8e8814a2f7f9cc832fedad8ea5a926181d60d\n","Successfully built imageio\n","Installing collected packages: imageio\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.34.2\n","    Uninstalling imageio-2.34.2:\n","      Successfully uninstalled imageio-2.34.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= \"3.4\", but you have imageio 2.4.0 which is incompatible.\n","scikit-image 0.23.2 requires imageio>=2.33, but you have imageio 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed imageio-2.4.0\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n","Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Collecting tf-agents[reverb]\n","  Downloading tf_agents-0.19.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n","Collecting gym<=0.23.0,>=0.17.0 (from tf-agents[reverb])\n","  Downloading gym-0.23.0.tar.gz (624 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.26.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n","Collecting typing-extensions==4.5.0 (from tf-agents[reverb])\n","  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n","Collecting pygame==2.1.3 (from tf-agents[reverb])\n","  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n","Collecting tensorflow-probability~=0.23.0 (from tf-agents[reverb])\n","  Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl.metadata (13 kB)\n","Collecting rlds (from tf-agents[reverb])\n","  Downloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl.metadata (1.7 kB)\n","Collecting dm-reverb~=0.14.0 (from tf-agents[reverb])\n","  Downloading dm_reverb-0.14.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (17 kB)\n","Collecting tensorflow~=2.15.0 (from tf-agents[reverb])\n","  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (0.1.8)\n","Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (1.5.2)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (18.1.1)\n","Collecting ml-dtypes~=0.3.1 (from tensorflow~=2.15.0->tf-agents[reverb])\n","  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (24.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (71.0.4)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.4.0)\n","Collecting wrapt>=1.11.1 (from tf-agents[reverb])\n","  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.37.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.64.1)\n","Collecting tensorboard<2.16,>=2.15 (from tensorflow~=2.15.0->tf-agents[reverb])\n","  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n","Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow~=2.15.0->tf-agents[reverb])\n","  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting keras<2.16,>=2.15.0 (from tensorflow~=2.15.0->tf-agents[reverb])\n","  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents[reverb]) (4.4.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-agents[reverb]) (0.44.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.2.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.0.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.14.0->tf-agents[reverb]) (5.9.5)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (5.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.2.2)\n","Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n","Downloading dm_reverb-0.14.0-cp310-cp310-manylinux2014_x86_64.whl (6.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tf_agents-0.19.0-py3-none-any.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: gym\n","  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697632 sha256=02c3d02066bfaa8adf828e030a35512f3830c1e44a7e1c6d9f82d633977fb325\n","  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n","Successfully built gym\n","Installing collected packages: wrapt, typing-extensions, tensorflow-probability, tensorflow-estimator, rlds, pygame, ml-dtypes, keras, gym, tf-agents, dm-reverb, tensorboard, tensorflow\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.16.0\n","    Uninstalling wrapt-1.16.0:\n","      Successfully uninstalled wrapt-1.16.0\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.12.2\n","    Uninstalling typing_extensions-4.12.2:\n","      Successfully uninstalled typing_extensions-4.12.2\n","  Attempting uninstall: tensorflow-probability\n","    Found existing installation: tensorflow-probability 0.24.0\n","    Uninstalling tensorflow-probability-0.24.0:\n","      Successfully uninstalled tensorflow-probability-0.24.0\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.6.0\n","    Uninstalling pygame-2.6.0:\n","      Successfully uninstalled pygame-2.6.0\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.4.0\n","    Uninstalling ml-dtypes-0.4.0:\n","      Successfully uninstalled ml-dtypes-0.4.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.4.1\n","    Uninstalling keras-3.4.1:\n","      Successfully uninstalled keras-3.4.1\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.17.0\n","    Uninstalling tensorboard-2.17.0:\n","      Successfully uninstalled tensorboard-2.17.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.17.0\n","    Uninstalling tensorflow-2.17.0:\n","      Successfully uninstalled tensorflow-2.17.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","sqlalchemy 2.0.32 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n","albucore 0.0.13 requires typing-extensions>=4.9.0, but you have typing-extensions 4.5.0 which is incompatible.\n","albumentations 1.4.13 requires typing-extensions>=4.9.0, but you have typing-extensions 4.5.0 which is incompatible.\n","pydantic 2.8.2 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n","pydantic-core 2.20.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n","tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.1 which is incompatible.\n","torch 2.3.1+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n","typeguard 4.3.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed dm-reverb-0.14.0 gym-0.23.0 keras-2.15.0 ml-dtypes-0.3.2 pygame-2.1.3 rlds-0.1.8 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 tensorflow-probability-0.23.0 tf-agents-0.19.0 typing-extensions-4.5.0 wrapt-1.14.1\n","Collecting pyglet\n","  Downloading pyglet-2.0.17-py3-none-any.whl.metadata (7.9 kB)\n","Downloading pyglet-2.0.17-py3-none-any.whl (936 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m936.6/936.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyglet\n","Successfully installed pyglet-2.0.17\n"]}]},{"cell_type":"code","source":["from __future__ import absolute_import, division, print_function\n","\n","import base64\n","import imageio\n","import IPython\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import PIL.Image\n","import pyvirtualdisplay\n","import reverb\n","\n","import tensorflow as tf\n","\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.drivers import py_driver\n","from tf_agents.environments import suite_gym\n","from tf_agents.environments import tf_py_environment\n","from tf_agents.eval import metric_utils\n","from tf_agents.metrics import tf_metrics\n","from tf_agents.networks import sequential\n","from tf_agents.policies import py_tf_eager_policy\n","from tf_agents.policies import random_tf_policy\n","from tf_agents.replay_buffers import reverb_replay_buffer\n","from tf_agents.replay_buffers import reverb_utils\n","from tf_agents.trajectories import trajectory\n","from tf_agents.trajectories import time_step as ts\n","from tf_agents.specs import tensor_spec\n","from tf_agents.utils import common"],"metadata":{"id":"a5xQJhTuxCu7","executionInfo":{"status":"ok","timestamp":1724303890631,"user_tz":-540,"elapsed":483,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Am7Ju0RhgL06","executionInfo":{"status":"ok","timestamp":1724302823755,"user_tz":-540,"elapsed":424,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}}},"outputs":[],"source":["import sys\n","from os import mkdir\n","from os.path import exists\n","from sys import exc_info\n","import numpy as np\n","from enum import Enum, IntEnum, auto\n","from collections import Counter\n","from functools import reduce\n","# !pip install gym\n","import gym\n","from gym import spaces\n","from gym.utils import seeding\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Flatten\n","from keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","import io\n","\n","# import keras\n","# !pip install wandb\n","\n","# !pip install dopamine-rl\n","# !pip install keras-rl2\n","# !pip install keras-rl\n","# !pip install keras\n","# !git clone https://github.com/matthiasplappert/keras-rl.git\n","# !pip install ./keras-rl\n","# !pip install rl\n","# import importlib\n","# import kerasrl/rl\n","# from kerasrl.rl.agents.dqn import DQNAgent\n","# from kerasrl.rl.policy import BoltzmannQPolicy\n","# from kerasrl.rl.memory import SequentialMemory\n","# from kerasrl.rl.callbacks import TrainIntervalLogger, TrainEpisodeLogger\n","# from kerasrl.rl.callbacks import TrainIntervalLogger, TrainEpisodeLogger"]},{"cell_type":"code","source":["num_iterations = 20000 #反復回数 @param {type:\"integer\"}\n","# num_iterations = 200 #反復回数 @param {type:\"integer\"}\n","\n","initial_collect_steps = 1000  #初期収集ステップ数 @param {type:\"integer\"}\n","collect_steps_per_iteration = 1  #反復あたりの収集ステップ数 @param {type:\"integer\"}\n","replay_buffer_max_length = 100000  #再生バッファの最大長 @param {type:\"integer\"}\n","\n","batch_size = 64  #バッチサイズ @param {type:\"integer\"}\n","learning_rate = 1e-3  #学習率 @param {type:\"number\"}\n","log_interval = 200  #ログの間隔 @param {type:\"integer\"}\n","\n","num_eval_episodes = 10  #評価エピソード数 @param {type:\"integer\"}\n","eval_interval = 1000  #評価間隔 @param {type:\"integer\"}"],"metadata":{"id":"RH9MJeVb69Y4","executionInfo":{"status":"ok","timestamp":1724314802892,"user_tz":-540,"elapsed":433,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}}},"execution_count":232,"outputs":[]},{"cell_type":"markdown","source":["# 中間"],"metadata":{"id":"iTEcQ82vw6mM"}},{"cell_type":"code","source":["class Hand(IntEnum):\n","    R = 0\n","    S = 1\n","    P = 2\n","\n","class Strategy(Enum):\n","    RANDOM = auto()\n","    HUMAN = auto()\n","    PUMPKIN = auto()\n","    MAXIMUM = auto()\n","    NASH = auto()\n","    MEMORY = auto()\n","    MAJORITY = auto()"],"metadata":{"id":"WYnoTFzHgQfT","executionInfo":{"status":"ok","timestamp":1724302831992,"user_tz":-540,"elapsed":428,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class RandomStrategy:\n","    hands = [Hand.R, Hand.S, Hand.P]\n","\n","    def __init__(self, np_random):\n","        self.np_random = np_random\n","\n","    def get_hand(self, user_hand):\n","        return self.np_random.choice(RandomStrategy.hands)\n","\n","    def reset(self):\n","        pass\n","\n","class HumanStrategy:\n","    # 芳沢光雄(2009)『ジャンケンに関する研究結果』による\n","    ratio = [0.350, 0.317, 0.333]\n","    hands = [Hand.R, Hand.S, Hand.P]\n","\n","    def __init__(self, np_random):\n","        self.np_random = np_random\n","\n","    def get_hand(self, user_hand):\n","        return self.np_random.choice(HumanStrategy.hands, p=HumanStrategy.ratio)\n","\n","    def reset(self):\n","        pass\n","\n","class NashStrategy:\n","    ratio = [0.4, 0.4, 0.2]\n","    hands = [Hand.R, Hand.S, Hand.P]\n","\n","    def __init__(self, np_random):\n","        self.np_random = np_random\n","\n","    def get_hand(self, user_hand):\n","        return self.np_random.choice(NashStrategy.hands, p=NashStrategy.ratio)\n","\n","    def reset(self):\n","        pass"],"metadata":{"id":"GHMUfybCgkZz","executionInfo":{"status":"ok","timestamp":1724302834293,"user_tz":-540,"elapsed":550,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class RSP125(gym.Env):\n","    metadata = {\n","        'render.modes': ['human', 'ansi']\n","    }\n","\n","    action_space = spaces.Discrete(3) # グー、チョキ、パー\n","    reward_range = [0, 5] # 報酬の最小値、最大値\n","    # observation_space = spaces.Box(low=0, high=100, shape=(4,), dtype='float32')\n","    observation_space = spaces.Box(low=0, high=2, shape=([1,10]), dtype='int64')\n","\n","    reward_space = spaces.Box(low=0, high=500, shape=(1,), dtype=np.float32)\n","    # 'step_type': TensorShape([]),\n","    # 'reward': TensorShape([]),\n","    # 'discount':\n","\n","    # def time_step_spec(self):\n","    #     return self._time_step_spec\n","\n","    def __init__(self, strategy=Strategy.HUMAN, goal=100):\n","        super().__init__()\n","\n","        self.seed()\n","\n","        self.strategy = None\n","        if strategy == Strategy.RANDOM:\n","            self.strategy = RandomStrategy(self.np_random)\n","        elif strategy == Strategy.HUMAN:\n","            self.strategy = HumanStrategy(self.np_random)\n","        elif strategy == Strategy.PUMPKIN:\n","            self.strategy = PumpkinStrategy(self.np_random)\n","        elif strategy == Strategy.MAXIMUM:\n","            self.strategy = MaximumStrategy(self.np_random)\n","        elif strategy == Strategy.NASH:\n","            self.strategy = NashStrategy(self.np_random)\n","        elif strategy == Strategy.MAJORITY:\n","            self.strategy = MajorityStrategy(self.np_random)\n","        elif strategy == Strategy.MEMORY:\n","            self.strategy = MemoryStrategy(self.np_random)\n","\n","        self.goal = goal\n","        self.user_hands = []\n","        self.enemy_hands = []\n","        self.reset()\n","\n","        # # 勝手に作成中\n","        self.observation_space = spaces.Box(low=0, high=2, shape=(10,), dtype=np.int64)\n","        self.action_space = spaces.Discrete(3)\n","        self._time_step_spec = ts.TimeStep(\n","            step_type=tensor_spec.TensorSpec(shape=(1,), dtype=tf.int32, name='step_type'),\n","            reward=tensor_spec.TensorSpec(shape=(1,), dtype=tf.float32, name='reward'),\n","            discount=tensor_spec.BoundedTensorSpec(shape=(1,), dtype=tf.float32, minimum=0.0, maximum=1.0, name='discount'),\n","            observation=tensor_spec.BoundedTensorSpec(shape=(1, 10), dtype=tf.int64, minimum=0, maximum=2, name='observation')\n","        )\n","        # self._time_step_spec = ts.TimeStep(\n","        #     step_type=tensor_spec.BoundedTensorSpec(shape=[1], dtype=tf.int32, minimum=0, maximum=2),\n","        #     reward=tensor_spec.TensorSpec(shape=[1], dtype=tf.float32),\n","        #     discount=tensor_spec.BoundedTensorSpec(shape=[1], dtype=tf.float32, minimum=0.0, maximum=1.0),\n","        #     observation=tensor_spec.TensorSpec(shape=[10], dtype=tf.float32)\n","        # )\n","\n","    def reset(self):\n","        self.user_position = 0\n","        self.enemy_position  = 0\n","        self.user_hand = Hand.R   # 最初はグー\n","        self.user_positions = [self.user_position]\n","        self.enemy_hand = Hand.R  # 最初はグー\n","        self.enemy_positions = [self.enemy_position]\n","        self.strategy.reset()\n","        self.done = False\n","        # observation = [self.user_position, self.enemy_position, self.user_hand, self.enemy_hand]\n","        self.observation = np.array([2, 0, 1, 2, 1, 0, 2, 1, 0, 1], dtype=np.int64)\n","        # self.observation = ts.restart([2, 0, 1, 2, 1, 0, 2, 1, 0, 1])\n","        return self.observation\n","        # return self.observation, 0, self.done, {}\n","\n","    def seed(self, seed=None):\n","        self.np_random, seed = seeding.np_random(seed) # シードから乱数ジェネレーターを生成し、ジェネレーターとシードを返す\n","        return [seed]\n","\n","    def update_observation(self, userhand, enemyhand, obs):\n","      obs = obs[2:]\n","      obs = np.append(obs,userhand)\n","      obs = np.append(obs, enemyhand)\n","      return obs\n","\n","    def step(self, action):\n","        # self.enemy_hand = self.strategy.get_hand(self.user_hand)\n","        self.enemy_hand = Hand(self.strategy.get_hand(self.user_hand))\n","        self.enemy_hands += [self.enemy_hand]\n","        self.user_hand = Hand(action)\n","        self.user_hands += [self.user_hand]\n","\n","        # あいこの場合\n","        if self.user_hand == self.enemy_hand:\n","            # observation = [self.user_position, self.enemy_position, self.user_hand, self.enemy_hand]\n","            self.observation = self.update_observation(action, self.strategy.get_hand(self.user_hand), self.observation)\n","            reward = 0\n","            self.done = False\n","\n","        # プレーヤーが勝つ場合\n","        elif self.user_hand == Hand.R and self.enemy_hand == Hand.S:\n","            self.user_position += 1\n","            # observation = [self.user_position, self.enemy_position, self.user_hand, self.enemy_hand]\n","            self.observation = self.update_observation(action, self.strategy.get_hand(self.user_hand), self.observation)\n","            if self.user_position >= 100:\n","                reward = 1000\n","                self.done = True\n","            else:\n","                reward = 0\n","                self.done = False\n","\n","        elif self.user_hand == Hand.S and self.enemy_hand == Hand.P:\n","            self.user_position += 2\n","            # observation = [self.user_position, self.enemy_position, self.user_hand, self.enemy_hand]\n","            self.observation = self.update_observation(action, self.strategy.get_hand(self.user_hand), self.observation)\n","            if self.user_position >= 100:\n","                reward = 1000\n","                self.done = True\n","            else:\n","                reward = 0\n","                self.done = False\n","\n","        elif self.user_hand == Hand.P and self.enemy_hand == Hand.R:\n","            self.user_position += 5\n","            # observation = [self.user_position, self.enemy_position, self.user_hand, self.enemy_hand]\n","            self.observation = self.update_observation(action, self.strategy.get_hand(self.user_hand), self.observation)\n","            if self.user_position >= 100:\n","                reward = 1000\n","                self.done = True\n","            else:\n","                reward = 0\n","                self.done = False\n","\n","        # 敵が勝つ場合\n","        elif self.user_hand == Hand.R and self.enemy_hand == Hand.P:\n","            self.enemy_position += 5\n","            # observation = [self.user_position, self.enemy_position, self.user_hand, self.enemy_hand]\n","            self.observation = self.update_observation(action, self.strategy.get_hand(self.user_hand), self.observation)\n","            if self.enemy_position >= 100:\n","                reward = 0\n","                self.done = True\n","            else:\n","                reward = 0\n","                self.done = False\n","\n","        elif self.user_hand == Hand.S and self.enemy_hand == Hand.R:\n","            self.enemy_position += 1\n","            # observation = [self.user_position, self.enemy_position, self.user_hand, self.enemy_hand]\n","            self.observation = self.update_observation(action, self.strategy.get_hand(self.user_hand), self.observation)\n","            if self.enemy_position >= 100:\n","                reward = 0\n","                self.done = True\n","            else:\n","                reward = 0\n","                self.done = False\n","\n","        elif self.user_hand == Hand.P and self.enemy_hand == Hand.S:\n","            self.enemy_position += 2\n","            # observation = [self.user_position, self.enemy_position, self.user_hand, self.enemy_hand]\n","            self.observation = self.update_observation(action, self.strategy.get_hand(self.user_hand), self.observation)\n","            if self.enemy_position >= 100:\n","                reward = 0\n","                self.done = True\n","            else:\n","                reward = 0\n","                self.done = False\n","\n","        self.user_positions += [self.user_position]\n","        self.enemy_positions += [self.enemy_position]\n","\n","        return self.observation, reward, self.done, {}\n","        # obs, reward, done, info = step(action)   infoは好きに使える辞書型\n","\n","    def e2s(self, e):\n","        if e == Hand.R:\n","            s = 'グー'\n","        elif e == Hand.S:\n","            s = 'チョキ'\n","        elif e == Hand.P:\n","            s = 'パー'\n","        return s\n","\n","    def render(self, mode='human', close=False):\n","        if mode == 'ansi':\n","            outfile = io.StringIO()\n","        elif mode == 'human':\n","            outfile = sys.stdout\n","        else:\n","            # just raise an exception\n","            super().render(mode=mode)\n","\n","        strs = [i for i in range(5)]\n","        strs[0] = '\\n'\n","        strs[1] = 'プレーヤーの手：' + self.e2s(self.user_hand) + '、'\n","        strs[2] = '敵の手：' + self.e2s(self.enemy_hand) + '\\n'\n","        strs[3] = 'プレーヤーの位置：' + str(self.user_position) + '、'\n","        strs[4] = '敵の位置：' + str(self.enemy_position) + '\\n'\n","        outfile.write(''.join(strs))\n","        return outfile\n","\n","    def close(self):\n","        # just return\n","        super().close()\n","\n","    def play(self):\n","        self.reset()\n","        while not self.done:\n","            print('')\n","            print('じゃーんけーん')\n","            self.step(self.__input())\n","            self.render()\n","        print('また遊ぼうね！')\n","\n","    def __input(self):\n","        while True:\n","            print('[グー：0、チョキ：1、パー：2]')\n","            i = input()\n","            if i in ['0', '1', '2']:\n","                break\n","        return int(i)\n","\n","\n","    # 勝手に追加\n","    def observation_spec(self):\n","        return self._time_step_spec.observation\n","    def action_spec(self):\n","        return tensor_spec.BoundedTensorSpec(shape=(3,), dtype=tf.int64, minimum=0, maximum=2)\n","    def _reset(self):\n","        observation = np.array( np.random.randint(0, 3, size=(1, 10)) )\n","        return ts.restart(observation)\n","    def _step(self, action):\n","        observation = np.random.randint(0, 3, size=(1, 10))\n","        reward = np.random.random(size=(1,))\n","        step_type = np.array([ts.StepType.MID], dtype=np.int)\n","        discount = np.array([1.0], dtype=np.float32)\n","        return ts.TimeStep(\n","            step_type=step_type,\n","            reward=reward,\n","            discount=discount,\n","            observation=observation\n","        )\n","    def time_step_spec(self):\n","        return self._time_step_spec"],"metadata":{"id":"mBe9aXLdh0_3","executionInfo":{"status":"ok","timestamp":1724314807703,"user_tz":-540,"elapsed":660,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}}},"execution_count":233,"outputs":[]},{"cell_type":"code","source":["env = suite_gym.wrap_env(RSP125())\n","print('Observation Spec:')\n","print(env.observation_space)\n","print('Reward Spec:')\n","print(env.reward_range)\n","print('Action Spec:')\n","print(env.action_space)\n","env.observation_space.sample()\n","\n","print()\n","print()\n","\n","# print(env.time_step_spec_custom())\n","print(env.time_step_spec())\n","print(\"-- env.reset() --\")\n","print(env.reset())\n","\n","print()\n","print()\n","\n","time_step_test = env.reset()\n","print('Time step:')\n","print(time_step_test)\n","# env.reset()\n","\n","action = np.array(1, dtype=np.int64)\n","next_time_step_test = env.step(action)\n","print('Next time step:')\n","print(next_time_step_test)\n","print(\"[MyPosi,エネposi,MyHan,エネhan]\")\n","\n","print()\n","\n","action = np.array(2, dtype=np.int64)\n","next_next_time_step_test = env.step(action)\n","print('Next time step:')\n","print(next_next_time_step_test)\n","print(\"[MyPosi,エネposi,MyHan,エネhan]\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kUrFNZzBdMD-","executionInfo":{"status":"ok","timestamp":1724314811482,"user_tz":-540,"elapsed":519,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}},"outputId":"23bcdaca-bf7e-4029-a990-54044b511c90"},"execution_count":234,"outputs":[{"output_type":"stream","name":"stdout","text":["Observation Spec:\n","Box(0, 2, (10,), int64)\n","Reward Spec:\n","[0, 5]\n","Action Spec:\n","Discrete(3)\n","\n","\n","TimeStep(\n","{'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'),\n"," 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n"," 'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n"," 'observation': BoundedArraySpec(shape=(10,), dtype=dtype('int64'), name='observation', minimum=0, maximum=2)})\n","-- env.reset() --\n","TimeStep(\n","{'step_type': array(0, dtype=int32),\n"," 'reward': array(0., dtype=float32),\n"," 'discount': array(1., dtype=float32),\n"," 'observation': array([2, 0, 1, 2, 1, 0, 2, 1, 0, 1])})\n","\n","\n","Time step:\n","TimeStep(\n","{'step_type': array(0, dtype=int32),\n"," 'reward': array(0., dtype=float32),\n"," 'discount': array(1., dtype=float32),\n"," 'observation': array([2, 0, 1, 2, 1, 0, 2, 1, 0, 1])})\n","Next time step:\n","TimeStep(\n","{'step_type': array(1, dtype=int32),\n"," 'reward': array(0., dtype=float32),\n"," 'discount': array(1., dtype=float32),\n"," 'observation': array([1, 2, 1, 0, 2, 1, 0, 1, 1, 0])})\n","[MyPosi,エネposi,MyHan,エネhan]\n","\n","Next time step:\n","TimeStep(\n","{'step_type': array(1, dtype=int32),\n"," 'reward': array(0., dtype=float32),\n"," 'discount': array(1., dtype=float32),\n"," 'observation': array([1, 0, 2, 1, 0, 1, 1, 0, 2, 1])})\n","[MyPosi,エネposi,MyHan,エネhan]\n"]}]},{"cell_type":"code","source":["# 環境のリセットとステップを実行\n","initial_observation = env.reset()\n","print(\"Initial Observation:\", initial_observation)\n","\n","action = env.action_space.sample()\n","print(\"Sample Action:\", action)\n","\n","observation, reward, done, info = env.step(action)\n","print(\"Next Observation:\", observation)\n","print(\"Reward:\", reward)\n","print(\"Done:\", done)\n","print(\"Info:\", info)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_TgY-U8mIm-4","executionInfo":{"status":"ok","timestamp":1724314821734,"user_tz":-540,"elapsed":2,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}},"outputId":"98b643e4-d029-45cf-fae9-a629ec7cab65"},"execution_count":235,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial Observation: TimeStep(\n","{'step_type': array(0, dtype=int32),\n"," 'reward': array(0., dtype=float32),\n"," 'discount': array(1., dtype=float32),\n"," 'observation': array([2, 0, 1, 2, 1, 0, 2, 1, 0, 1])})\n","Sample Action: 1\n","Next Observation: 1\n","Reward: 0.0\n","Done: 1.0\n","Info: [1 2 1 0 2 1 0 1 1 1]\n"]}]},{"cell_type":"code","source":["train_py_env = RSP125()\n","eval_py_env = RSP125()\n","\n","wrap_train_py_env = suite_gym.wrap_env(train_py_env)\n","wrap_eval_py_env = suite_gym.wrap_env(eval_py_env)\n","\n","train_env = tf_py_environment.TFPyEnvironment(wrap_train_py_env)\n","eval_env = tf_py_environment.TFPyEnvironment(wrap_eval_py_env)"],"metadata":{"id":"f-RR78CNvaIR","executionInfo":{"status":"ok","timestamp":1724314824044,"user_tz":-540,"elapsed":420,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}}},"execution_count":236,"outputs":[]},{"cell_type":"markdown","source":["# エージェント"],"metadata":{"id":"9ZM62LHvwrsu"}},{"cell_type":"code","execution_count":238,"metadata":{"id":"TgkdEPg_muzV","executionInfo":{"status":"ok","timestamp":1724314829833,"user_tz":-540,"elapsed":2,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}}},"outputs":[],"source":["fc_layer_params = (100, 50)\n","# action_tensor_spec = tensor_spec.from_spec(env.action_space)\n","# num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n","num_actions = 3\n","\n","# Define a helper function to create Dense layers configured with the right\n","# activation and kernel initializer.\n","def dense_layer(num_units):\n","  return tf.keras.layers.Dense(\n","      num_units,\n","      activation=tf.keras.activations.relu,\n","      kernel_initializer=tf.keras.initializers.VarianceScaling(\n","          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n","\n","# QNetwork consists of a sequence of Dense layers followed by a dense layer\n","# with `num_actions` units to generate one q_value per available action as\n","# its output.\n","dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n","q_values_layer = tf.keras.layers.Dense(\n","    num_actions,\n","    activation=None,\n","    kernel_initializer=tf.keras.initializers.RandomUniform(\n","        minval=-0.03, maxval=0.03),\n","    bias_initializer=tf.keras.initializers.Constant(-0.2))\n","# q_net = sequential.Sequential(dense_layers + [q_values_layer])\n","# q_net = sequential.Sequential(dense_layers + [q_values_layer, tf.keras.layers.Lambda(lambda x: tf.squeeze(x, axis=-2))])\n","q_net = sequential.Sequential(dense_layers + [q_values_layer])\n"]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","train_step_counter = tf.Variable(0)\n","\n","print(train_env.time_step_spec())\n","print(train_env.action_spec())\n","\n","agent = dqn_agent.DqnAgent(\n","    train_env.time_step_spec(),\n","    train_env.action_spec(),\n","    q_network=q_net,\n","    optimizer=optimizer,\n","    td_errors_loss_fn=common.element_wise_squared_loss,\n","    train_step_counter=train_step_counter\n","  )\n","\n","agent.initialize()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RefPhSRzrx50","executionInfo":{"status":"ok","timestamp":1724314830405,"user_tz":-540,"elapsed":5,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}},"outputId":"6bd3e1e7-8f30-4759-b648-4e8b851ae917"},"execution_count":239,"outputs":[{"output_type":"stream","name":"stdout","text":["TimeStep(\n","{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n"," 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n"," 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n"," 'observation': BoundedTensorSpec(shape=(10,), dtype=tf.int64, name='observation', minimum=array(0), maximum=array(2))})\n","BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(2))\n"]}]},{"cell_type":"markdown","source":["# ポリシー"],"metadata":{"id":"LBgN5TED-eyr"}},{"cell_type":"code","source":["eval_policy = agent.policy # 評価とデプロイに使用される主なポリシー。\n","collect_policy = agent.collect_policy #  データ収集に使用される補助的なポリシー。"],"metadata":{"id":"laZ9_er3-iWt","executionInfo":{"status":"ok","timestamp":1724314832369,"user_tz":-540,"elapsed":2,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}}},"execution_count":240,"outputs":[]},{"cell_type":"code","source":["train_env.step(np.array(2, dtype=np.int64))\n","# random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec()) #　オリジン\n","random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())"],"metadata":{"id":"b-UwIEpL-6OH","executionInfo":{"status":"ok","timestamp":1724314832793,"user_tz":-540,"elapsed":6,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}}},"execution_count":241,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOlnlRRsUbxP"},"source":["ポリシーから行動を取得するには、`policy.action(time_step)`メソッドを呼び出します。`time_step`には、環境からの観測が含まれています。このメソッドは、3 つのコンポーネントを持つ名前付きタプルである`PolicyStep`を返します。\n","\n","- `action` — 実行する行動（ここでは`0`または`1`)\n","- `state` — ステートフルポリシー（RNNベース）に使用\n","- `info` — 行動のログ確率などの補助データ"]},{"cell_type":"code","source":["example_environment = tf_py_environment.TFPyEnvironment(suite_gym.wrap_env(RSP125()))\n","time_step = example_environment.reset()\n","print(time_step)\n","random_policy.action(time_step)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dKBzueg-iUh","executionInfo":{"status":"ok","timestamp":1724314832793,"user_tz":-540,"elapsed":6,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}},"outputId":"d4c9edfb-b9a5-4d72-f8d0-c10c2c669846"},"execution_count":242,"outputs":[{"output_type":"stream","name":"stdout","text":["TimeStep(\n","{'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n"," 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n"," 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n"," 'observation': <tf.Tensor: shape=(1, 10), dtype=int64, numpy=array([[2, 0, 1, 2, 1, 0, 2, 1, 0, 1]])>})\n"]},{"output_type":"execute_result","data":{"text/plain":["PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=())"]},"metadata":{},"execution_count":242}]},{"cell_type":"markdown","source":["# 指標と評価"],"metadata":{"id":"mwwPI9WQvZp5"}},{"cell_type":"code","source":["#@test {\"skip\": true}\n","def compute_avg_return(environment, policy, num_episodes=10):\n","\n","  total_return = 0.0\n","  for _ in range(num_episodes):\n","\n","    time_step = environment.reset()\n","    episode_return = 0.0\n","\n","    while not time_step.is_last():\n","      action_step = policy.action(time_step)\n","      time_step = environment.step(action_step.action)\n","      episode_return += time_step.reward\n","    total_return += episode_return\n","\n","  avg_return = total_return / num_episodes\n","  return avg_return.numpy()[0]\n","\n","\n","# See also the metrics module for standard implementations of different metrics.\n","# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"],"metadata":{"id":"Pwb8PaX3-iRC","executionInfo":{"status":"ok","timestamp":1724314850834,"user_tz":-540,"elapsed":1,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}}},"execution_count":245,"outputs":[]},{"cell_type":"code","source":["compute_avg_return(eval_env, random_policy, num_eval_episodes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"bBooXEXw-iIy","executionInfo":{"status":"ok","timestamp":1724314857013,"user_tz":-540,"elapsed":3436,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}},"outputId":"97892861-57e8-4576-8318-d3828530081b"},"execution_count":246,"outputs":[{"output_type":"execute_result","data":{"text/plain":["500.0"]},"metadata":{},"execution_count":246}]},{"cell_type":"markdown","source":["# データ収集"],"metadata":{"id":"7S-Fia1wwgll"}},{"cell_type":"code","source":["table_name = 'uniform_table'\n","replay_buffer_signature = tensor_spec.from_spec(\n","      agent.collect_data_spec)\n","replay_buffer_signature = tensor_spec.add_outer_dim(\n","    replay_buffer_signature)\n","\n","table = reverb.Table(\n","    table_name,\n","    max_size=replay_buffer_max_length,\n","    sampler=reverb.selectors.Uniform(),\n","    remover=reverb.selectors.Fifo(),\n","    rate_limiter=reverb.rate_limiters.MinSize(1),\n","    signature=replay_buffer_signature)\n","\n","reverb_server = reverb.Server([table])\n","\n","replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n","    agent.collect_data_spec,\n","    table_name=table_name,\n","    sequence_length=2,\n","    local_server=reverb_server)\n","\n","rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n","  replay_buffer.py_client,\n","  table_name,\n","  sequence_length=2)\n","\n","print(agent.collect_data_spec)\n","print(agent.collect_data_spec._fields)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxPxLTl1xAvq","executionInfo":{"status":"ok","timestamp":1724314860107,"user_tz":-540,"elapsed":636,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}},"outputId":"181ea09e-bf58-4577-e09f-7e0376c03c87"},"execution_count":247,"outputs":[{"output_type":"stream","name":"stdout","text":["Trajectory(\n","{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n"," 'observation': BoundedTensorSpec(shape=(10,), dtype=tf.int64, name='observation', minimum=array(0), maximum=array(2)),\n"," 'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(2)),\n"," 'policy_info': (),\n"," 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n"," 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n"," 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))})\n","('step_type', 'observation', 'action', 'policy_info', 'next_step_type', 'reward', 'discount')\n"]}]},{"cell_type":"code","source":["print(ts.restart(train_py_env.reset()), \"\\n\")\n","\n","py_driver.PyDriver(\n","    env,\n","    py_tf_eager_policy.PyTFEagerPolicy(random_policy, use_tf_function=True),\n","    [rb_observer],\n","    max_steps=initial_collect_steps).run(\n","        ts.restart(train_py_env.reset())\n","        # time_step: tf_agents.trajectories.TimeStep,\n","        # policy_state: tf_agents.typing.types.NestedArray = ()\n","      )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DYTHOce2wkXU","executionInfo":{"status":"ok","timestamp":1724314863009,"user_tz":-540,"elapsed":2903,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}},"outputId":"5ff27746-4a3b-46cf-c038-b7cd70e3dc07"},"execution_count":248,"outputs":[{"output_type":"stream","name":"stdout","text":["TimeStep(\n","{'step_type': array(0, dtype=int32),\n"," 'reward': array(0., dtype=float32),\n"," 'discount': array(1., dtype=float32),\n"," 'observation': array([2, 0, 1, 2, 1, 0, 2, 1, 0, 1])}) \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["(TimeStep(\n"," {'step_type': array(1, dtype=int32),\n","  'reward': array(0., dtype=float32),\n","  'discount': array(1., dtype=float32),\n","  'observation': array([2, 0, 2, 0, 2, 2, 1, 0, 1, 0])}),\n"," ())"]},"metadata":{},"execution_count":248}]},{"cell_type":"code","source":["dataset = replay_buffer.as_dataset(\n","    num_parallel_calls=3,\n","    sample_batch_size=batch_size,\n","    num_steps=2).prefetch(3)\n","\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8bTLjsNDJVw","executionInfo":{"status":"ok","timestamp":1724314863010,"user_tz":-540,"elapsed":9,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}},"outputId":"408c8480-4f70-416d-9ba6-ade494cf4464"},"execution_count":249,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_PrefetchDataset element_spec=(Trajectory(\n","{'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n"," 'observation': TensorSpec(shape=(64, 2, 10), dtype=tf.int64, name=None),\n"," 'action': TensorSpec(shape=(64, 2), dtype=tf.int64, name=None),\n"," 'policy_info': (),\n"," 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n"," 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n"," 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)))>"]},"metadata":{},"execution_count":249}]},{"cell_type":"code","source":["iterator = iter(dataset)\n","print(iterator)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iIeMGL7OI-ZE","executionInfo":{"status":"ok","timestamp":1724314863010,"user_tz":-540,"elapsed":9,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}},"outputId":"088a6122-890f-4f01-9e42-8ca1a56ba4d2"},"execution_count":250,"outputs":[{"output_type":"stream","name":"stdout","text":["<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7be9b4b0e0e0>\n"]}]},{"cell_type":"markdown","source":["# eエージェント　トレーニング"],"metadata":{"id":"ZeMzsbBcv_5z"}},{"cell_type":"code","source":["\n","try:\n","  %%time\n","except:\n","  pass\n","\n","# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n","agent.train = common.function(agent.train)\n","\n","# Reset the train step.\n","agent.train_step_counter.assign(0)\n","\n","# Evaluate the agent's policy once before training.\n","avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n","returns = [avg_return]\n","\n","# Reset the environment.\n","# time_step = train_py_env.reset()\n","time_step = ts.restart(train_py_env.reset())\n","print(time_step)\n","\n","# Create a driver to collect experience.\n","collect_driver = py_driver.PyDriver(\n","    env,\n","    py_tf_eager_policy.PyTFEagerPolicy(\n","      agent.collect_policy, use_tf_function=True),\n","    [rb_observer],\n","    max_steps=collect_steps_per_iteration)\n","\n","for _ in range(num_iterations):\n","\n","  # Collect a few steps and save to the replay buffer.\n","  time_step, _ = collect_driver.run(time_step)\n","\n","  # Sample a batch of data from the buffer and update the agent's network.\n","  experience, unused_info = next(iterator)\n","  train_loss = agent.train(experience).loss\n","\n","  step = agent.train_step_counter.numpy()\n","\n","  if step % log_interval == 0:\n","    print('step = {0}: loss = {1}'.format(step, train_loss))\n","\n","  if step % eval_interval == 0:\n","    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n","    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n","    returns.append(avg_return)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Df4FXOWSwM6I","executionInfo":{"status":"error","timestamp":1724315044701,"user_tz":-540,"elapsed":178719,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}},"outputId":"9ba06cc1-52e4-4776-87eb-1ee65b400466"},"execution_count":251,"outputs":[{"output_type":"stream","name":"stdout","text":["TimeStep(\n","{'step_type': array(0, dtype=int32),\n"," 'reward': array(0., dtype=float32),\n"," 'discount': array(1., dtype=float32),\n"," 'observation': array([2, 0, 1, 2, 1, 0, 2, 1, 0, 1])})\n","step = 200: loss = 20799.09765625\n","step = 400: loss = 2125820.5\n","step = 600: loss = 32101610.0\n","step = 800: loss = 25628328.0\n","step = 1000: loss = 134094416.0\n","step = 1000: Average Return = 1000.0\n","step = 1200: loss = 30358918.0\n","step = 1400: loss = 45809224.0\n","step = 1600: loss = 87353896.0\n","step = 1800: loss = 161586784.0\n","step = 2000: loss = 39744456.0\n","step = 2000: Average Return = 1000.0\n","step = 2200: loss = 138104800.0\n","step = 2400: loss = 179948288.0\n","step = 2600: loss = 131347800.0\n","step = 2800: loss = 25003680.0\n","step = 3000: loss = 146783408.0\n","step = 3000: Average Return = 1000.0\n","step = 3200: loss = 123951744.0\n","step = 3400: loss = 17189488.0\n","step = 3600: loss = 54288856.0\n","step = 3800: loss = 78850608.0\n","step = 4000: loss = 71569104.0\n","step = 4000: Average Return = 1000.0\n","step = 4200: loss = 9151482.0\n","step = 4400: loss = 30566674.0\n","step = 4600: loss = 6597471.0\n","step = 4800: loss = 4476847.5\n","step = 5000: loss = 28928224.0\n","step = 5000: Average Return = 1000.0\n","step = 5200: loss = 4975197.0\n","step = 5400: loss = 4577557.5\n","step = 5600: loss = 3213231.0\n","step = 5800: loss = 21912596.0\n","step = 6000: loss = 8192607.0\n","step = 6000: Average Return = 1000.0\n","step = 6200: loss = 1007556.8125\n","step = 6400: loss = 5616926.5\n","step = 6600: loss = 494852.1875\n","step = 6800: loss = 4513129.5\n","step = 7000: loss = 304658.75\n","step = 7000: Average Return = 1000.0\n","step = 7200: loss = 1149076.125\n","step = 7400: loss = 1076161.375\n","step = 7600: loss = 529378.875\n","step = 7800: loss = 522842.125\n","step = 8000: loss = 335664.34375\n","step = 8000: Average Return = 1000.0\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-251-d984878c00c1>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m# Collect a few steps and save to the replay buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/drivers/py_driver.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m       \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m       \u001b[0mnext_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/policies/py_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    167\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/policies/py_tf_eager_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_nested_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Avoid passing numpy arrays to avoid retracing of the tf.function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mpolicy_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy_action_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mprovided\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   \"\"\"\n\u001b[0;32m--> 631\u001b[0;31m   return nest_util.map_structure(\n\u001b[0m\u001b[1;32m    632\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1064\u001b[0m   \"\"\"\n\u001b[1;32m   1065\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_core_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_data_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1104\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_conversion.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m   \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m   return convert_to_tensor_v2(\n\u001b[0m\u001b[1;32m    162\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_conversion.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;31m# preferred_dtype = preferred_dtype or dtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    172\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    333\u001b[0m                                          as_ref=False):\n\u001b[1;32m    334\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;31m# Register the conversion function for the \"unconvertible\" types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[0;32m--> 271\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    272\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    282\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m   const_tensor = ops._create_graph_constant(  # pylint: disable=protected-access\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    294\u001b[0m ) -> ops._EagerTensorBase:\n\u001b[1;32m    295\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# 対戦\n","https://qiita.com/tanuk1647/items/7b8c2f0d09330cbfacd2#%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%B3%E3%83%BC%E3%83%89"],"metadata":{"id":"_-s895jTMGwJ"}},{"cell_type":"code","source":["def visualize_hands(env):\n","    c = Counter(env.user_hands)\n","    plt.figure()\n","    x = np.array([c[Hand.GOO], c[Hand.CHOKI], c[Hand.PAH]])\n","    label = ['グー', 'チョキ', 'パー']\n","    plt.pie(x, labels=label, counterclock=False, startangle=90, labeldistance=0.8, autopct=\"%1.1f%%\")\n","    plt.axis('equal')\n","    plt.title('プレーヤーの手')\n","\n","    c = Counter(env.enemy_hands)\n","    plt.figure()\n","    x = np.array([c[Hand.GOO], c[Hand.CHOKI], c[Hand.PAH]])\n","    label = ['グー', 'チョキ', 'パー']\n","    plt.pie(x, labels=label, counterclock=False, startangle=90, labeldistance=0.8, autopct=\"%1.1f%%\")\n","    plt.axis('equal')\n","    plt.title('敵の手')\n","\n","\n","def visualize_positions(env):\n","    plt.figure()\n","    x = range(1, len(env.user_positions) + 1)\n","    u = env.user_positions\n","    e = env.enemy_positions\n","    plt.plot(x, u, marker='.', label='プレーヤーの位置')\n","    plt.plot(x, e, marker='.', label='敵の位置')\n","    plt.legend(loc='best', fontsize=10)\n","    plt.grid()\n","    plt.xlabel('steps')\n","    plt.ylabel('position')\n","    plt.show()"],"metadata":{"id":"NN0u_E44wiz8","executionInfo":{"status":"ok","timestamp":1724315110006,"user_tz":-540,"elapsed":589,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}}},"execution_count":252,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RojVMgaKMPfi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 皺寄せ"],"metadata":{"id":"Sh1tCsCI-kId"}},{"cell_type":"code","source":["class TrainIntervalLogger2():\n","    def __init__(self, interval=10000):\n","        super().__init__(interval=interval)\n","        self.records = {}\n","\n","    def on_train_begin(self, logs):\n","        super().on_train_begin(logs)\n","        self.records['interval'] = []\n","        self.records['episode_reward'] = []\n","        for metrics_name in self.metrics_names:\n","            self.records[metrics_name] = []\n","\n","    def on_step_begin(self, step, logs):\n","        if self.step % self.interval == 0:\n","            if len(self.episode_rewards) > 0:\n","                self.records['interval'].append(self.step // self.interval)\n","                self.records['episode_reward'].append(np.mean(self.episode_rewards))\n","                metrics = np.array(self.metrics)\n","                assert metrics.shape == (self.interval, len(self.metrics_names))\n","                if not np.isnan(metrics).all():  # not all values are means\n","                    means = np.nanmean(self.metrics, axis=0)\n","                    assert means.shape == (len(self.metrics_names),)\n","                    for name, mean in zip(self.metrics_names, means):\n","                        self.records[name].append(mean)\n","        super().on_step_begin(step, logs)\n","\n","class DQNRSP125:\n","    # 重み保存先\n","    weightdir = './data'\n","    weightfile = './data/dqn_{}_{}_weights.h5'\n","\n","    # モデルの初期化\n","    def __init__(self, strategy=Strategy.HUMAN, recycle=True):\n","        print('モデルを作成します。')\n","        self.train_interval_logger = None\n","\n","        # Get the environment and extract the number of actions.\n","        self.env = RSP125(strategy=strategy)\n","        self.env_name = 'janken_125'\n","        self.weightfile = DQNRSP125.weightfile.format(self.env_name, str(strategy))\n","        self.nb_actions = self.env.action_space.n\n","\n","        # Next, we build a very simple model.\n","        self.model = Sequential()\n","        self.model.add(Flatten(input_shape=(1,) + self.env.observation_space.shape))\n","        self.model.add(Dense(128))\n","        self.model.add(Activation('relu'))\n","        self.model.add(Dense(self.nb_actions))\n","        self.model.add(Activation('linear'))\n","        #print(self.model.summary())\n","\n","        # Finally, we configure and compile our agent.\n","        # You can use every built-in Keras optimizer and even the metrics!\n","        memory = SequentialMemory(limit=50000, window_length=1)\n","        policy = BoltzmannQPolicy(tau=1.)\n","        self.dqn = DQNAgent(model=self.model, nb_actions=self.nb_actions, memory=memory,\n","                            nb_steps_warmup=1000, target_model_update=1e-2, policy=policy)\n","        self.dqn.compile(Adam(lr=1e-3), metrics=[])\n","\n","        self.__istrained = False\n","        print('モデルを作成しました。')\n","\n","        if recycle:\n","            if exists(self.weightfile):\n","                try:\n","                    print('訓練済み重みを読み込みます。')\n","                    self.dqn.load_weights(self.weightfile)\n","                    self.__istrained = True\n","                    print('訓練済み重みを読み込みました。')\n","                    return None\n","                except:\n","                    print('訓練済み重みの読み込み中にエラーが発生しました。')\n","                    print('Unexpected error:', exc_info()[0])\n","                    raise\n","            else:\n","                print('訓練済み重みが存在しません。訓練を行ってください。')\n","\n","    # 訓練\n","    def train(self, nb_steps=30000, verbose=1, visualize=False, log_interval=3000):\n","        if self.__istrained:\n","            raise RuntimeError('このモデルは既に訓練済みです。')\n","\n","        print('訓練を行うので、お待ちください。')\n","\n","        # 訓練実施\n","        # Okay, now it's time to learn something!\n","        # We visualize the training here for show, but this slows down training quite a lot.\n","        # You can always safely abort the training prematurely using Ctrl + C.\n","        callbacks = []\n","        if verbose == 1:\n","            self.train_interval_logger = TrainIntervalLogger2()\n","            callbacks.append(self.train_interval_logger)\n","            verbose = 0\n","        elif verbose > 1:\n","            callbacks.append(TrainEpisodeLogger())\n","            verbose = 0\n","\n","        hist = self.dqn.fit(self.env, nb_steps=nb_steps,\n","                            callbacks=callbacks, verbose=verbose,\n","                            visualize=visualize, log_interval=log_interval)\n","        self.__istrained = True\n","\n","        if self.train_interval_logger is not None:\n","            # 訓練状況の可視化\n","            interval = self.train_interval_logger.records['interval']\n","            episode_reward = self.train_interval_logger.records['episode_reward']\n","            mean_q = self.train_interval_logger.records['mean_q']\n","            if len(interval) > len(mean_q):\n","                mean_q = np.pad(mean_q, [len(interval) - len(mean_q), 0], \"constant\")\n","            plt.figure()\n","            plt.plot(interval, episode_reward, marker='.', label='報酬')\n","            plt.plot(interval, mean_q, marker='.', label='Q値')\n","            plt.legend(loc='best', fontsize=10)\n","            plt.grid()\n","            plt.xlabel('interval')\n","            plt.ylabel('score')\n","            plt.xticks(np.arange(min(interval),\n","                                 max(interval) + 1,\n","                                 (max(interval) - min(interval))//7))\n","            plt.show()\n","\n","        # 重みの保存\n","        if not exists(DQNJankenGlico.weightdir):\n","            try:\n","                mkdir(DQNJankenGlico.weightdir)\n","            except:\n","                print('重み保存フォルダの作成中にエラーが発生しました。')\n","                print('Unexpected error:', exc_info()[0])\n","                raise\n","        try:\n","            # After training is done, we save the final weights.\n","            self.dqn.save_weights(self.weightfile, overwrite=True)\n","        except:\n","            print('重みの保存中にエラーが発生しました。')\n","            print('Unexpected error:', exc_info()[0])\n","            raise\n","\n","        return hist\n","\n","    # テスト\n","    def test(self, nb_episodes=10, visualize=True, verbose=1):\n","        # Finally, evaluate our algorithm for 5 episodes.\n","        hist = self.dqn.test(self.env, nb_episodes=nb_episodes,\n","                             verbose=verbose, visualize=visualize)\n","        return hist\n","\n","def visualize_hands(env):\n","    c = Counter(env.user_hands)\n","    plt.figure()\n","    x = np.array([c[Hand.R], c[Hand.S], c[Hand.P]])\n","    label = ['グー', 'チョキ', 'パー']\n","    plt.pie(x, labels=label, counterclock=False, startangle=90, labeldistance=0.8, autopct=\"%1.1f%%\")\n","    plt.axis('equal')\n","    plt.title('プレーヤーの手')\n","\n","    c = Counter(env.enemy_hands)\n","    plt.figure()\n","    x = np.array([c[Hand.R], c[Hand.S], c[Hand.P]])\n","    label = ['グー', 'チョキ', 'パー']\n","    plt.pie(x, labels=label, counterclock=False, startangle=90, labeldistance=0.8, autopct=\"%1.1f%%\")\n","    plt.axis('equal')\n","    plt.title('敵の手')\n","\n","\n","def visualize_positions(env):\n","    plt.figure()\n","    x = range(1, len(env.user_positions) + 1)\n","    u = env.user_positions\n","    e = env.enemy_positions\n","    plt.plot(x, u, marker='.', label='プレーヤーの位置')\n","    plt.plot(x, e, marker='.', label='敵の位置')\n","    plt.legend(loc='best', fontsize=10)\n","    plt.grid()\n","    plt.xlabel('steps')\n","    plt.ylabel('position')\n","    plt.show()"],"metadata":{"id":"PpmiUrZQl07p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d = DQNRSP125(strategy=Strategy.RANDOM, recycle=False)\n","h = d.train(nb_steps=40000, log_interval=2000, verbose=1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"id":"QNQzlr5-BqSh","executionInfo":{"status":"error","timestamp":1724094283324,"user_tz":-540,"elapsed":405,"user":{"displayName":"藤森大地","userId":"16785286525306946888"}},"outputId":"49453ad6-7b40-4e9e-8fc4-2192b2eeff0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["モデルを作成します。\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'SequentialMemory' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-cef57740bb63>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNRSP125\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRANDOM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-94054419dcc3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, strategy, recycle)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Finally, we configure and compile our agent.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# You can use every built-in Keras optimizer and even the metrics!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoltzmannQPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         self.dqn = DQNAgent(model=self.model, nb_actions=self.nb_actions, memory=memory,\n","\u001b[0;31mNameError\u001b[0m: name 'SequentialMemory' is not defined"]}]},{"cell_type":"markdown","source":["# メモ"],"metadata":{"id":"Lp-l0gmI-mV4"}},{"cell_type":"markdown","source":["作りたい環境<br>\n","入力：\n","- 5回前の自分の手(0,1,2)\n","- 5回前の相手の手(0,1,2)\n","-  /\n","-  /\n","-  /\n","- 1回前の自分の手(0,1,2)\n","- 1回前の相手の手(0,1,2)\n","3^10\n","出力：\n","- 自分の出す手(0,1,2)"],"metadata":{"id":"AQCSikj5j9T1"}}]}